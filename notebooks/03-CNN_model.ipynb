{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30cbf344",
   "metadata": {},
   "source": [
    "# CNN model\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e47803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /mnt/arkk/kaggle/ariel-data-challenge\n"
     ]
    }
   ],
   "source": [
    "# Set notebook root to project root\n",
    "from helper_functions import set_project_root\n",
    "\n",
    "set_project_root()\n",
    "\n",
    "# Standard library imports\n",
    "import datetime\n",
    "import random\n",
    "import shutil\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Local imports\n",
    "import configuration as config\n",
    "\n",
    "wavelengths = 283\n",
    "sample_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc846628",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### 1.1. Load planet list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2828f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1100 planets in training data.\n"
     ]
    }
   ],
   "source": [
    "# Load corrected/extracted data for a sample planet\n",
    "with h5py.File(f'{config.PROCESSED_DATA_DIRECTORY}/train.h5', 'r') as hdf:\n",
    "    planet_ids = list(hdf.keys())\n",
    "\n",
    "print(f'Found {len(planet_ids)} planets in training data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ce92f",
   "metadata": {},
   "source": [
    "### 1.2. Split planets into training & validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb4e151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training planets: 550\n",
      "Validation planets: 550\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(planet_ids)\n",
    "\n",
    "training_planet_ids = planet_ids[:len(planet_ids) // 2]\n",
    "validation_planet_ids = planet_ids[len(planet_ids) // 2:]\n",
    "\n",
    "print(f'Training planets: {len(training_planet_ids)}')\n",
    "print(f'Validation planets: {len(validation_planet_ids)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e56c6c8",
   "metadata": {},
   "source": [
    "## 2. Data generator\n",
    "\n",
    "### 2.1. Data loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a770dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(planet_ids: list, data_file: str, sample_size: int = 100):\n",
    "    '''Generator that yields signal, spectrum pairs for training/validation/testing.\n",
    "\n",
    "    Args:\n",
    "        planet_ids (list): List of planet IDs to include in the generator.\n",
    "        data_file (str): Path to the HDF5 file containing the data.\n",
    "        sample_size (int, optional): Number of frames to draw from each planet. Defaults to 100.\n",
    "    '''\n",
    "\n",
    "    with h5py.File(data_file, 'r') as hdf:\n",
    "\n",
    "        while True:\n",
    "            np.random.shuffle(planet_ids)\n",
    "            \n",
    "            for planet_id in planet_ids:\n",
    "\n",
    "                signal = hdf[planet_id]['signal'][:]\n",
    "                spectrum = hdf[planet_id]['spectrum'][:]\n",
    "\n",
    "                indices = random.sample(range(signal.shape[0]), sample_size)\n",
    "                sample = signal[sorted(indices), :]\n",
    "\n",
    "                yield sample, spectrum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e961fa07",
   "metadata": {},
   "source": [
    "### 2.2. Prefill the arguments to `data_loader()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d22cc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_generator = partial(\n",
    "    data_loader,\n",
    "    planet_ids=training_planet_ids,\n",
    "    data_file=f'{config.PROCESSED_DATA_DIRECTORY}/train.h5',\n",
    "    sample_size=config.SAMPLES\n",
    ")\n",
    "\n",
    "validation_data_generator = partial(\n",
    "    data_loader,\n",
    "    planet_ids=validation_planet_ids,\n",
    "    data_file=f'{config.PROCESSED_DATA_DIRECTORY}/train.h5',\n",
    "    sample_size=config.SAMPLES\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e49150",
   "metadata": {},
   "source": [
    "### 2.3. Create TF datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9bd92d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 07:13:58.622703: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10774 MB memory:  -> device: 0, name: Tesla K80, pci bus id: 0000:05:00.0, compute capability: 3.7\n",
      "2025-09-18 07:13:58.623847: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10774 MB memory:  -> device: 1, name: Tesla K80, pci bus id: 0000:06:00.0, compute capability: 3.7\n"
     ]
    }
   ],
   "source": [
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    training_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(sample_size, wavelengths), dtype=tf.float64),\n",
    "        tf.TensorSpec(shape=(wavelengths), dtype=tf.float64)\n",
    "    )\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    validation_data_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(sample_size, wavelengths), dtype=tf.float64),\n",
    "        tf.TensorSpec(shape=(wavelengths), dtype=tf.float64)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d586e99",
   "metadata": {},
   "source": [
    "## 3. CNN\n",
    "\n",
    "### 3.1. Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e2bd52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(\n",
    "        samples: int=config.SAMPLES,\n",
    "        wavelengths: int=config.WAVELENGTHS,\n",
    "        learning_rate: float=config.LEARNING_RATE,\n",
    "        l1: float=config.L1_PENALTY,\n",
    "        l2: float=config.L2_PENALTY,\n",
    "        filter_nums=config.FILTER_NUMS,\n",
    "        filter_size=config.FILTER_SIZE\n",
    ") -> tf.keras.Model:\n",
    "\n",
    "    '''Builds the convolutional neural network regression model'''\n",
    "\n",
    "    # Set-up the L1L2 for the dense layers\n",
    "    regularizer = tf.keras.regularizers.L1L2(l1=l1, l2=l2)\n",
    "\n",
    "    # Define the model layers in order\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input((samples,wavelengths,1)),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filter_nums[0],\n",
    "            filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filter_nums[1],\n",
    "            filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Conv2D(\n",
    "            filter_nums[2],\n",
    "            filter_size,\n",
    "            padding='same',\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.MaxPooling2D(),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(\n",
    "            128,\n",
    "            kernel_regularizer=regularizer,\n",
    "            activation='relu',\n",
    "        ),\n",
    "        tf.keras.layers.Dense(wavelengths, activation='relu')\n",
    "    ])\n",
    "\n",
    "    # Define the optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # Compile the model, specifying the type of loss to use during training and any extra\n",
    "    # metrics to evaluate\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.MeanSquaredError(name='MSE'),\n",
    "        metrics=[tf.keras.metrics.RootMeanSquaredError(name='RMSE')]\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e89a4b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 100, 283, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 50, 141, 32)      0         \n",
      " )                                                               \n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 100, 283, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 50, 141, 32)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 50, 141, 64)       18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 25, 70, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 25, 70, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 35, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 53760)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               6881408   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 283)               36507     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,010,587\n",
      "Trainable params: 7,010,587\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = compile_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83900f61",
   "metadata": {},
   "source": [
    "### 3.2. Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a575a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with tensorboard log directory\n",
    "try:\n",
    "    shutil.rmtree(f'{config.TENSORBOARD_LOG_DIR}')\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "Path(config.TENSORBOARD_LOG_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Set tensorboard callback\n",
    "log_dir = config.TENSORBOARD_LOG_DIR + f'{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=1\n",
    ")\n",
    "\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    min_delta=0.000005,\n",
    "    mode='min',\n",
    "    verbose=1,\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d85d366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-18 07:14:00.846970: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2025-09-18 07:14:02.141957: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f6cf000ec10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-09-18 07:14:02.142000: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
      "2025-09-18 07:14:02.142014: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (1): Tesla K80, Compute Capability 3.7\n",
      "2025-09-18 07:14:02.151922: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-09-18 07:14:02.307438: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 21s 322ms/step - loss: 8.2522e-04 - RMSE: 0.0287 - val_loss: 1.7590e-04 - val_RMSE: 0.0133\n",
      "Epoch 2/20\n",
      "50/50 [==============================] - 16s 320ms/step - loss: 1.8805e-04 - RMSE: 0.0137 - val_loss: 1.4463e-04 - val_RMSE: 0.0120\n",
      "Epoch 3/20\n",
      "50/50 [==============================] - 16s 325ms/step - loss: 1.6962e-04 - RMSE: 0.0130 - val_loss: 1.3846e-04 - val_RMSE: 0.0118\n",
      "Epoch 4/20\n",
      "50/50 [==============================] - 17s 338ms/step - loss: 1.5237e-04 - RMSE: 0.0123 - val_loss: 1.3229e-04 - val_RMSE: 0.0115\n",
      "Epoch 5/20\n",
      "50/50 [==============================] - 18s 368ms/step - loss: 1.5163e-04 - RMSE: 0.0123 - val_loss: 1.2319e-04 - val_RMSE: 0.0111\n",
      "Epoch 6/20\n",
      "50/50 [==============================] - 16s 326ms/step - loss: 1.4058e-04 - RMSE: 0.0119 - val_loss: 1.2323e-04 - val_RMSE: 0.0111\n",
      "Epoch 7/20\n",
      "50/50 [==============================] - ETA: 0s - loss: 1.4310e-04 - RMSE: 0.0120"
     ]
    }
   ],
   "source": [
    "training_results = model.fit(\n",
    "  training_dataset.batch(config.BATCH_SIZE),\n",
    "  validation_data=validation_dataset.batch(config.BATCH_SIZE),\n",
    "  epochs=config.EPOCHS,\n",
    "  steps_per_epoch=config.STEPS,\n",
    "  validation_steps=config.STEPS,\n",
    "  verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6b67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-up a 1x2 figure for accuracy and binary cross-entropy\n",
    "fig, axs=plt.subplots(1,2, figsize=(8,4))\n",
    "\n",
    "# Add the main title\n",
    "fig.suptitle('CNN training curves', size='large')\n",
    "\n",
    "# Plot training and validation MSE\n",
    "axs[0].set_title('Mean squared error')\n",
    "axs[0].plot(np.array(training_results.history['loss']), label='Training')\n",
    "axs[0].plot(np.array(training_results.history['val_loss']), label='Validation')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].set_ylabel('MSE')\n",
    "axs[0].legend(loc='upper left')\n",
    "\n",
    "# Plot training and validation binary cross-entropy\n",
    "axs[1].set_title('Root mean squared error')\n",
    "axs[1].plot(training_results.history['RMSE'])\n",
    "axs[1].plot(training_results.history['val_RMSE'])\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].set_ylabel('RMSE')\n",
    "\n",
    "# Show the plot\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.8.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
